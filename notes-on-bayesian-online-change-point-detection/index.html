<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Notes on Bayesian Online Change Point Detection -</title>
<meta name="description" content="In this post I am going to explain the mathematical details behind the graphical model Bayesian Online Change Point Detection introduced in (Adams &amp; MacKay, 2007). This model can be used to detect different type of change-points and has known many extensions over the last few years. Given a time series, we are interested in detecting structural changes as new data comes in 1. A great explanation of this paper can be found in (Gundersen, 2022)]. You might ask, why reading this post then? There is three main reasons. First, I will try to clarify parts that were not trivial to me despite reading the paper and the post above (e.g. a tiny mistake was made in the math derivation). Second and most importantly the paper only provides a way to calculate the probability that there was a change-point k units of time ago, but it does not propose a strategy to segment the time-series. I will suggest an algorithm to perform an optimal segmentation at a given time t. Finally, I will show how we can use BOCPD to detect trend changes.                 Online detection is opposed to Offline change-point detection. In &#8617;">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="Notes on Bayesian Online Change Point Detection">
<meta property="og:url" content="/notes-on-bayesian-online-change-point-detection/">


  <meta property="og:description" content="In this post I am going to explain the mathematical details behind the graphical model Bayesian Online Change Point Detection introduced in (Adams &amp; MacKay, 2007). This model can be used to detect different type of change-points and has known many extensions over the last few years. Given a time series, we are interested in detecting structural changes as new data comes in 1. A great explanation of this paper can be found in (Gundersen, 2022)]. You might ask, why reading this post then? There is three main reasons. First, I will try to clarify parts that were not trivial to me despite reading the paper and the post above (e.g. a tiny mistake was made in the math derivation). Second and most importantly the paper only provides a way to calculate the probability that there was a change-point k units of time ago, but it does not propose a strategy to segment the time-series. I will suggest an algorithm to perform an optimal segmentation at a given time t. Finally, I will show how we can use BOCPD to detect trend changes.                 Online detection is opposed to Offline change-point detection. In &#8617;">







  <meta property="article:published_time" content="2022-03-01T00:00:00+00:00">





  

  


<link rel="canonical" href="/notes-on-bayesian-online-change-point-detection/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Hamza FILALI",
      "url": "/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/88x88.png" alt=""></a>
        
        <a class="site-title" href="/">
          
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/change-point-detection-blog/">Change Point Detection Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/appendix/">Appendix</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="/" itemprop="url"></a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Notes on Bayesian Online Change Point Detection">
    <meta itemprop="description" content="In this post I am going to explain the mathematical details behind the graphical model Bayesian OnlineChange Point Detection introduced in (Adams &amp; MacKay, 2007).This model can be used to detect different type of change-points and has known many extensions over the last few years.Given a time series, we are interested in detecting structural changesas new data comes in 1.A great explanation of this paper can be found in (Gundersen, 2022)].You might ask, why reading this post then? There is three main reasons.First, I will try to clarify parts that were not trivial to me despitereading the paper and the post above (e.g. a tiny mistake was made in the mathderivation).Second and most importantly the paper only provides a way to calculatethe probability that there was a change-point k units of time ago, butit does not propose a strategy to segment the time-series. I willsuggest an algorithm to perform an optimal segmentation at a given time t.Finally, I will show how we can use BOCPD to detect trend changes.            Online detection is opposed to Offline change-point detection. In &#8617;      ">
    <meta itemprop="datePublished" content="2022-03-01T00:00:00+00:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="/notes-on-bayesian-online-change-point-detection/" class="u-url" itemprop="url">Notes on Bayesian Online Change Point Detection
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#bocpd">BOCPD</a><ul><li><a href="#introducing-the-main-ingredient-run-length">Introducing the main ingredient: Run-length</a></li><li><a href="#conditional-dependence-structure-assumptions">Conditional dependence structure assumptions</a></li><li><a href="#underlying-distribution-assumptions">Underlying distribution assumptions</a><ul><li><a href="#change-point-prior-model">Change-point prior model</a></li><li><a href="#predictive-model-assumptions">Predictive Model assumptions</a></li></ul></li></ul></li><li><a href="#how-can-we-use-the-output-of-bocpd-to-detect-changes">How can we use the output of BOCPD to detect changes?</a><ul><li><a href="#an-offline-strategy-a-viterbi-like-algorithm">An offline strategy: a viterbi-like algorithm</a></li><li><a href="#an-online-strategy">An online strategy</a></li></ul></li><li><a href="#references">References</a></li></ul>

            </nav>
          </aside>
        
        <p>In this post I am going to explain the mathematical details behind the graphical model <em>Bayesian Online
Change Point Detection</em> introduced in <a class="citation" href="#adams2007bayesian">(Adams &amp; MacKay, 2007)</a>.
This model can be used to detect different type of change-points and has known many extensions over the last few years.<br />
Given a time series, we are interested in detecting structural changes
as new data comes in <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.<br />
A great explanation of this paper can be found in <a class="citation" href="#gundersen">(Gundersen, 2022)</a>].<br />
You might ask, why reading this post then? There is three main reasons.<br />
First, I will try to clarify parts that were not trivial to me despite
reading the paper and the post above (e.g. a tiny mistake was made in the math
derivation).<br />
Second and most importantly the paper only provides a way to calculate
the probability that there was a change-point k units of time ago, but
it does not propose a strategy to segment the time-series. I will
suggest an algorithm to perform an optimal segmentation at a given time t.
Finally, I will show how we can use BOCPD to detect trend changes.</p>

<figure>
<a name="graphicalmodel">
</a>
<img src="../images/bocpd_graphical_model.png" alt="Trulli" style="width:100%" />
<figcaption align="center"><b>Graphical model associated to BOCPD. Source
<a class="citation" href="#kim2015reading">(Kim &amp; Choi, 2015)</a></b></figcaption>
</figure>

<h1 id="bocpd">BOCPD</h1>

<p>Let \(\{x_{1}, ... x_{T}\}\in \mathbf{R}^{T\times d}\) denote the time
series of interest.</p>

<h2 id="introducing-the-main-ingredient-run-length">Introducing the main ingredient: Run-length</h2>

<p>Let \(r_{t}\), the <em>run-length</em>, be a random variable that represent the
time since the last change-point. For instance, \(r_{t} = j\) implies that
the last change-point was at time t-j.<br />
It follows that \(r_{t} \in \mathbf{R}^{t}\) and \(r_{t} = \begin{cases}
0 \text{     if change-point at time $t$} \\
r_{t-1} + 1 \text{     else} \end{cases}\)</p>

<p><br />
In the BOCPD model, our main interest lies in calculating the run-length
posterior distribution \(P(r_{t}|x_{1:t})\).<br />
In order to do compute it we need to make some assumptions around:</p>

<ol>
  <li>
    <p>the conditional dependence structure between \(\{x_{1}, ... x_{T}\}\)
and \(\{r_{1}, ... r_{T}\}\)</p>
  </li>
  <li>
    <p>the underlying distribution of \(\{x_{1}, ... x_{T}\}\) and
\(\{r_{1}, ... r_{T}\}\)</p>
  </li>
</ol>

<h2 id="conditional-dependence-structure-assumptions">Conditional dependence structure assumptions</h2>

<p>The assumptions that we are going to explain in more details are
outlined in the <a href="#graphicalmodel">graphical representation above</a>.</p>

<p><ins>Assumption 1:</ins> We assume that \(r_{t}\) is conditionally
independent of everything else given \(r_{t-1}\). In particular, \(P(r_{t} | r_{t-1}, x_{1:t}) = P(r_{t} | r_{t-1})\)\</p>

<p><ins>Assumption 2:</ins> We assume that the current observation only
depends on the past observations associated to the current partition
defined be the run-length, i.e.
\(P(x_{t} | r_{t-1}=r, x_{1:t-1}) = P(x_{t} | x_{t-r-1:t-1})\)</p>

<p>With these two assumptions, we can derive our main objective
\(P(r_{t}|x_{1:t})\).<br />
Given that \(P(r_{t}|x_{1:t}) = \frac{P(r_{t},x_{1:t})}{\sum_{r'_{t}}P(r'_{t},x_{1:t})}\),
we just need to solve for \(P(r_{t},x_{1:t})\).</p>

<p>\(\begin{aligned}
P(r_{t},x_{1:t}) =&amp; \sum_{r_{t-1}} P(r_{t}, r_{t-1}, x_{1:t})  \text{      (by marginalizing over $r_{t-1}$)}\\ =&amp; \sum_{r_{t-1}} P(r_{t}, x_{t} | r_{t-1}, x_{1:t-1})P(r_{t-1}, x_{1:t-1}) \text{     (by bayes rule)}\\ =&amp;  \sum_{r_{t-1}} P(r_{t}| x_{t} , r_{t-1}, x_{1:t-1})  P( x_{t} | r_{t-1}, x_{1:t-1}) P(r_{t-1}, x_{1:t-1}) \text{      (by bayes rule)} \\ =&amp;  \sum_{r_{t-1}} P(r_{t}|  r_{t-1})  P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) P(r_{t-1}, x_{1:t-1}) \quad \text{ (by assumption 1 and 2)}
\end{aligned}\)<br />
Three terms appear in the equations above:</p>

<p>\(P(r_{t},x_{1:t}) = \sum_{r_{t-1}} \underbrace{P(r_{t}|  r_{t-1})}_{\text{Change-point Prior}}   \underbrace{P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1})}_{\text{Predictive Model}} \underbrace{P(r_{t-1}, x_{1:t-1})}_{\text{Message}}\)<br />
We now need to make assumptions on the distribution of the Change-point
Prior and the Underlying Predictive Model in order to be able to
recursively derive the run-length posterior.</p>

<h2 id="underlying-distribution-assumptions">Underlying distribution assumptions</h2>

<h3 id="change-point-prior-model">Change-point prior model</h3>

<p>The transition probability \(P(r_{t}|r_{t-1})\) is assumed to follow the
below distribution:</p>

\[P(r_{t}|r_{t-1}) = \begin{cases}
H(r_{t-1} + 1) \quad \text{if   } r_t = 0
\\
1 - H(r_{t-1} + 1) \quad \text{if }r_t = r_{t-1} + 1
\\
0 \quad \text{otherwise}
\end{cases}\]

<p>where  \(H(\tau) = \frac{p_{g}(\tau)}{\sum_{t=\tau}^{\infty}p_{g}(t)}\) is the
hazard function<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> and \(p_{g}(t)\) represents the probability of a
segment of length t.</p>

<p>If we set \(p_{g}\) to follow a geometric
distribution with parameter \(\lambda\) then
\(H(\tau) = \frac{1}{\lambda}\). 
The prior \(\lambda\) encodes our belief on
the expected length of a segment, in other words our prior on the
average length of a partition.</p>

<h3 id="predictive-model-assumptions">Predictive Model assumptions</h3>

<p>The last part we need to compute is the predictive probability
\(P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1})\).<br />
<br />
<ins>Assumption 3:</ins> We assume that \(x_{t}\) follows a
distribution with parameter \(\eta\) (where \(\eta\) is a random quantity in
a bayesian framework). \(\eta\) is shared across all the \(x_{i}\) within a
segment (defined by the run-length).</p>

<p>Given our new point, we are trying to estimate the probability that this
new point belongs to the current partition: 
\(P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) = \int P( x_{t} | \eta, r_{t-1}, x_{t-1- r_{t-1}:t-1}) P( \eta | r_{t-1}, x_{t-1- r_{t-1}:t-1})d \eta\)<br />
<br />
<ins>Assumption 4:</ins>
\(x_{t} \perp\kern-5pt\perp  r_{t-1}, x_{t-1- r_{t-1}:t-1} |\eta\).<br />
<br />
Under this assumption, we obtain:</p>

<p>\(P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) = \int \underbrace{P( x_{t}| \eta)}_\text{underlying model}  \underbrace{P( \eta | r_{t-1}, x_{t-1- r_{t-1}:t-1})}_{\text{Posterior Distribution}}d \eta\)<br />
We now need to define the underlying distribution of our time series and the
distribution of the prior \(\eta\).</p>

<p>The law \(x_{t} | \eta\) that we choose defines the type of change-points
we wish to detect.<br />
If we are interested in detecting abrupt shifts in the mean, we could
then choose a Gaussian distribution.<br />
If we have count data, we could be tempted to choose a Poisson
distribution. If we have are interested in changes in the trend, we
could choose a Bayesian linear regression with time as a regressor.<br />
<br />
In <a class="citation" href="#adams2007bayesian">(Adams &amp; MacKay, 2007)</a>, \(x_{t}| \eta\) is restricted to come from
the Exponential Family and the prior \(P(\eta)\) is restricted to be a
conjugate prior (which means that
\(P( \eta | r_{t-1}, x_{t-1- r_{t-1}:t-1})\) and \(P(\eta)\) come from the
same family distribution).<br />
<br />
The choice of Exponential family + conjugate prior is motivated by a
trade-off between tractability of the equations and generalization over
the type of change-points that can be detected:</p>

<ol>
  <li>
    <p>The conjugate prior allows for a tractable derivation of the
posterior distribution as we obtain a closed-form of the posterior
distribution. The main derivation needed is on the hyper-parameters
associated with the posterior distribution. In the case of an
Exponential Family likelihood, the hyper-parameters have an easy
formula that can be updated sequentially, a very appealing property
in the case of sequential learning.</p>
  </li>
  <li>
    <p>The Exponential Family encompasses a lot of widely used family for
change-point detection problems (e.g. normal distribution, poisson
distribution, bayesian linear regression...).</p>
  </li>
  <li>
    <p>Most of the exponential family distributions lead to a closed form
of the predictive probability when the prior is conjugate, which
avoids us to resort to bayesian approximation techniques that are
computationally costly.</p>
  </li>
</ol>

<h1 id="how-can-we-use-the-output-of-bocpd-to-detect-changes">How can we use the output of BOCPD to detect changes?</h1>

<h2 id="an-offline-strategy-a-viterbi-like-algorithm">An offline strategy: a viterbi-like algorithm</h2>

<p>A natural criteria to find the best segmentation up until time t is to
find the Maximum A Posteriori sequence of run-length, i.e. the most
probable sequence of run-length:</p>

\[r^{*}_{1}, ... r^{*}_{t} = arg \max\limits_{r_{1}, ... r_{t}} P(r_{1}, ... r_{t} | x_{1}, ..., x_{t})\]

<p>By noticing that:  \(arg \max\limits_{r_{1}, ... r_{t}} P(r_{1}, ... r_{t} | x_{1}, ..., x_{t}) =  arg \max\limits_{r_{1}, ... r_{t}} P(r_{1}, ... r_{t} , x_{1}, ..., x_{t})\)
We will optimize \(P(r_{1}, ... r_{t} , x_{1}, ..., x_{t})\) through a dynamic programming algorithm.</p>

<p>This optimization formulation is the same as the Viterbi Algorithm used
for Hidden Markov Models.However, the structural dependencies are slightly different, hence the
recursion is slightly different.</p>

<p>Figure <a href="#graphicalmodel">1</a> illustrates the structural dependencies assumptions in BOCPD, which are used in the last line of the equation below.</p>

\[\begin{aligned}
    P(r_{1}, ..., r_{t} , x_{1}, ..., x_{t}) =&amp; P( r_{t} | r_{1}, ..., r_{t-1}, x_{1}, ..., x_{t}) P(r_{1}, ..., r_{t-1}, , x_{1}, ..., x_{t}) \\ =&amp; P( r_{t} | r_{1}, ..., r_{t-1}, x_{1}, ..., x_{t}) P( x_{t} | r_{1}, ..., r_{t-1}, x_{1}, ..., x_{t-1}) P(r_{1}, ..., r_{t-1} , x_{1}, ..., x_{t-1})
    \\ =&amp; P( r_{t} | r_{t-1}) P( x_{t} | r_{t-1}, x_{1}, ..., x_{t-1}) P(r_{1}, ..., r_{t-1} , x_{1}, ..., x_{t-1})
\end{aligned}\]

<p>Now let’s introduce a few notations:<br />
Let
\(\delta_{t}(j) \overset{\Delta}{=}  \log P(r_{1}, ..., r_{t}=j, x_{1}, ..., x_{t})\).<br />
Let \(A(i,j)  \overset{\Delta}{=} \log P(r_{t}=j|r_{t-1}=i)\).<br />
Let
\(\lambda(i)  \overset{\Delta}{=} \log P( x_{t} = i| r_{t-1}, x_{1}, ..., x_{t-1})\).</p>

<p>Using the equation above, we get:<br />
\(\delta_{t}(j) = \max\limits_{i} A(i,j) + \lambda(i) +\delta_{t-1}(i)\)</p>

<p>This recursion above allows us to compute the optimal sequence of
run-length at each time step.<br />
An important drawback is the time and space complexity of this
algorithm. The space complexity is \(O(T^{2})\) while the computational complexity is
\(O(T)\).</p>

<h2 id="an-online-strategy">An online strategy</h2>

<p>In the online change-point setting, we are above all interested in
detecting a new change-point with minimum delay (more than finding
retrospectively the optimal segmentation).<br />
An easy and natural strategy would be to detect an alert if the
run-length posterior distribution
\(P(r_{t} = l| x_{1}, ..., x_{t}) &gt; \text{threshold}\).<br />
There is two parameters to consider here. 
The parameter \(l\) directly encodes the detection delay with which we will flag a change-point. 
The threshold encodes the confidence degree with which we will flag a
change-point.<br />
One issue with this methodology is that we will generally detect
multiple consecutive change-points.</p>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="adams2007bayesian">Adams, R. P., &amp; MacKay, D. J. C. (2007). Bayesian online changepoint detection. <i>ArXiv Preprint ArXiv:0710.3742</i>.</span></li>
<li><span id="gundersen">Gundersen, G. (2022). <i>Bayesian Online Changepoint Detection</i>. https://gregorygundersen.com/blog/2019/08/13/bocd/</span></li>
<li><span id="kim2015reading">Kim, T., &amp; Choi, J. (2015). Reading documents for bayesian online change point detection. <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i>, 1610–1619.</span></li></ol>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Online detection is opposed to Offline change-point detection. In
the offline case, we are interested in identifying structural
changes after observing the entire time series. The online case aims
at identifying structural changes as new observations come in. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A term coming from the field of survival analysis <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#bayesian" class="page__taxonomy-item p-category" rel="tag">bayesian</a><span class="sep">, </span>
    
      <a href="/tags/#change-point" class="page__taxonomy-item p-category" rel="tag">change-point</a><span class="sep">, </span>
    
      <a href="/tags/#online-change-point" class="page__taxonomy-item p-category" rel="tag">online change-point</a>
    
    </span>
  </p>




        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-03-01T00:00:00+00:00">March 1, 2022</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Notes+on+Bayesian+Online+Change+Point+Detection%20%2Fnotes-on-bayesian-online-change-point-detection%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=%2Fnotes-on-bayesian-online-change-point-detection%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=%2Fnotes-on-bayesian-online-change-point-detection%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          
            
      </div>
    </div>
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Hamza FILALI. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    

  




<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" defer
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true
     },
     "HTML-CSS": { availableFonts: ["TeX"] }
   });
</script>

  </body>
</html>
