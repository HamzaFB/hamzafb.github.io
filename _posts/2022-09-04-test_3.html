<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="hamza.filalibaba" />
  <title>Notes on BOCPD</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notes on BOCPD</h1>
<p class="author">hamza.filalibaba</p>
<p class="date">March 2022</p>
</header>
<h1 id="introduction">Introduction</h1>
<p>In this post I am going to explain the graphical model <em>Bayesian
Online Change Point Detection</em> introduced in <span class="citation"
data-cites="adams2007bayesian"></span> that has known many extensions
over the last few years.<br />
Given a time series, we are interested in detecting structural changes
as new data comes in <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.<br />
A great explanation of this paper can be found in <span class="citation"
data-cites="gundersen"></span>.<br />
You might ask, why reading this post then? There is three main
reasons.<br />
First, I will try to clarify parts that were not trivial to me despite
reading the post above (e.g. a tiny mistake was made in the math
derivation).<br />
Second and most importantly the paper only provide a way to calculate
the probability that there was a change-point k units of time ago, but
it does not propose a strategy to segment the time-series. I will
propose as an extension<br />
Finally, I will show how we can use BOCPD to detect trend changes.</p>
<p><span id="graphical_model" label="graphical_model"></span></p>
<figure>
<img src="../_images/bocpd_graphical_model.png" id="graphical_model"
alt="Graphical model associated to BOCPD. Source " />
<figcaption aria-hidden="true">Graphical model associated to BOCPD.
Source <span class="citation"
data-cites="kim2015reading"></span></figcaption>
</figure>
<h1 id="bocpd">BOCPD</h1>
<p>Let <span class="math inline">\(\{x_{1}, ... x_{T}\}\in
\mathbf{R}^{T\times d}\)</span> denote the time series of interest.</p>
<h2 id="introducing-the-main-ingredient-run-length">Introducing the main
ingredient: Run-length</h2>
<p>Let <span class="math inline">\(r_{t}\)</span>, the
<em>run-length</em>, be a random variable that represent the time since
the last change-point. For instance, <span class="math inline">\(r_{t} =
j\)</span> implies that the last change-point was at time t-j.<br />
It follows that <span class="math inline">\(r_{t} \in
\mathbf{R}^{t}\)</span> and <span class="math display">\[r_{t} =
\begin{cases}
0 \quad \text{if change-point at time $t$} \\
r_{t-1} + 1 \quad \text{else} \end{cases}\]</span><br />
In the BOCPD model, our main interest lies in calculating the run-length
posterior distribution <span
class="math inline">\(P(r_{t}|x_{1:t})\)</span>.<br />
In order to do compute it we need to make some assumptions around:</p>
<ol>
<li><p>the conditional dependence structure between <span
class="math inline">\(\{x_{1}, ... x_{T}\}\)</span> and <span
class="math inline">\(\{r_{1}, ... r_{T}\}\)</span></p></li>
<li><p>the underlying distribution of <span
class="math inline">\(\{x_{1}, ... x_{T}\}\)</span> and <span
class="math inline">\(\{r_{1}, ... r_{T}\}\)</span></p></li>
</ol>
<h2 id="conditional-dependence-structure-assumptions">Conditional
dependence structure assumptions</h2>
<p>The assumptions that we are going to explain in more details are
outlined in the graphical representation above.<br />
<u>Assumption 1:</u> We assume that <span
class="math inline">\(r_{t}\)</span> is conditionally independent of
everything else given <span class="math inline">\(r_{t-1}\)</span>. In
other words <span class="math inline">\(P(r_{t} | r_{t-1}, x_{1:t}) =
P(r_{t} | r_{t-1})\)</span><br />
<u>Assumption 2:</u> We assume that the current observation only depends
on the past observations associated to the current partition defined be
the run-length, i.e. <span class="math inline">\(P(x_{t} | r_{t-1}=r,
x_{1:t-1}) = P(x_{t} | x_{t-r-1:t-1})\)</span><br />
With these two assumptions, we can derive our main objective <span
class="math inline">\(P(r_{t}|x_{1:t})\)</span>.<br />
Given that <span class="math inline">\(P(r_{t}|x_{1:t}) =
\frac{P(r_{t},x_{1:t})}{\sum_{r&#39;_{t}}P(r&#39;_{t},x_{1:t})}\)</span>,
we just need to solve for <span
class="math inline">\(P(r_{t},x_{1:t})\)</span>. <span
class="math display">\[\begin{aligned}
P(r_{t},x_{1:t}) =&amp; \sum_{r_{t-1}} P(r_{t}, r_{t-1},
x_{1:t})  \text{\quad (by marginalizing over $r_{t-1}$)}\\ =&amp;
\sum_{r_{t-1}} P(r_{t}, x_{t} | r_{t-1}, x_{1:t-1})P(r_{t-1}, x_{1:t-1})
\text{\quad (by bayes rule)}\\ =&amp;  \sum_{r_{t-1}} P(r_{t}| x_{t} ,
r_{t-1}, x_{1:t-1})  P( x_{t} | r_{t-1}, x_{1:t-1}) P(r_{t-1},
x_{1:t-1}) \text{\quad (by bayes rule)} \\ =&amp;  \sum_{r_{t-1}}
P(r_{t}|  r_{t-1})  P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) P(r_{t-1},
x_{1:t-1}) \text{\quad (by assumption 1 and 2)}
\end{aligned}\]</span><br />
Three terms appear in the equations above:<br />
<span class="math inline">\(P(r_{t},x_{1:t}) = \sum_{r_{t-1}}
\underbrace{P(r_{t}|  r_{t-1})}_{\text{Changepoint
Prior}}   \underbrace{P( x_{t} | r_{t-1}, x_{t-1-
r_{t-1}:t-1})}_{\text{Underlying Predictive Model}}
\underbrace{P(r_{t-1}, x_{1:t-1})}_{\text{Message}}\)</span><br />
We now need to make assumptions on the distribution of the Change-point
Prior and the Underlying Predictive Model in order to be able to
recursively derive the run-length posterior.</p>
<h2 id="underlying-distribution-assumptions">Underlying distribution
assumptions</h2>
<h3 id="change-point-prior-model">Change-point prior model</h3>
<p>The transition probability <span
class="math inline">\(P(r_{t}|r_{t-1})\)</span> is assumed to follow the
below distribution: <span class="math display">\[P(r_{t}|r_{t-1}) =
\begin{cases}
H(r_{t-1} + 1) \quad \text{if $r_t = 0$}
\\
1 - H(r_{t-1} + 1) \quad \text{if $r_t = r_{t-1} + 1$}
\\
0 \quad \text{otherwise}
\end{cases}\]</span> where <span class="math inline">\(H(\tau) =
\frac{p_{g}(\tau)}{\sum_{t=\tau}^{\infty}p_{g}(t)}\)</span> is the
hazard function<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> and <span
class="math inline">\(p_{g}(t)\)</span> represents the probability of a
segment of length t. If we set <span
class="math inline">\(p_{g}\)</span> to follow a geometric distribution
with parameter <span class="math inline">\(\lambda\)</span> then <span
class="math inline">\(H(\tau) = \frac{1}{\lambda}\)</span>. The prior
<span class="math inline">\(\lambda\)</span> encodes our belief on the
expected length of a segment, in other words our prior on the average
length of a partition.</p>
<h3 id="predictive-model-assumptions">Predictive Model assumptions</h3>
<p>The last part we need to compute is the predictive probability <span
class="math inline">\(P( x_{t} | r_{t-1}, x_{t-1-
r_{t-1}:t-1})\)</span>.<br />
<br />
<u>Assumption 3:</u> We assume that <span
class="math inline">\(x_{t}\)</span> follows a distribution with
parameter <span class="math inline">\(\eta\)</span> (where <span
class="math inline">\(\eta\)</span> is a random quantity in a bayesian
framework). <span class="math inline">\(\eta\)</span> is shared across
all the <span class="math inline">\(x_{i}\)</span> within a segment
(defined by the run-length).<br />
<br />
This assumption results from the product partition model.<br />
Given our new point, we are trying to estimate the probability that this
point belongs to the current partition.<br />
<span class="math inline">\(P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) =
\int P( x_{t} | \eta, r_{t-1}, x_{t-1- r_{t-1}:t-1}) P( \eta | r_{t-1},
x_{t-1- r_{t-1}:t-1})d \eta\)</span><br />
<br />
<u>Assumption 4:</u> <span class="math inline">\(x_{t}
\perp\kern-5pt\perp  r_{t-1}, x_{t-1- r_{t-1}:t-1} |\eta\)</span>.<br />
<br />
Under this assumption, we obtain:<br />
<span class="math inline">\(P( x_{t} | r_{t-1}, x_{t-1- r_{t-1}:t-1}) =
\int \underbrace{P( x_{t}| \eta)}_\text{underlying
model}  \underbrace{P( \eta | r_{t-1}, x_{t-1-
r_{t-1}:t-1})}_{\text{Posterior Distribution}}d \eta\)</span>.<br />
We now need to define the distribution underlying model and the
disribution of the prior <span
class="math inline">\(\eta\)</span>.<br />
The law <span class="math inline">\(x_{t} | \eta\)</span> that we choose
defines the type of change-points we wish to detect.<br />
If we are interested in detecting abrupt shifts in the mean, we could
then choose a Gaussian distribution.<br />
If we have count data, we could be tempted to choose a Poisson
distribution. If we have are interested in changes in the trend, we
could choose a Bayesian linear regression with time as a
regressor.<br />
<br />
In <span class="citation" data-cites="adams2007bayesian"></span>, <span
class="math inline">\(P( x_{t}| \eta)\)</span> is restricted to come
from the Exponential Family and the prior <span
class="math inline">\(P(\eta)\)</span> is restricted to be a conjugate
prior (which means that <span class="math inline">\(P( \eta | r_{t-1},
x_{t-1- r_{t-1}:t-1})\)</span> and <span
class="math inline">\(P(\eta)\)</span> come from the same family
distribution).<br />
<br />
The choice of Exponential family + conjugate prior is motivated by a
trade-off between tractability of the equations and generalization over
the type of change-points that can be detected:</p>
<ol>
<li><p>The conjugate prior allows for a tractable derivation of the
posterior distribution as we obtain a closed-form of the posterior
distribution. The main derivation needed is on the hyper-parameters
associated with the posterior distribution. In the case of an
Exponential Family likelihood, the hyper-parameters have an easy formula
that can be updated sequentially, a very appealing property in the case
of sequential learning.</p></li>
<li><p>The Exponential Family encompasses a lot of widely used family
for change-point detection problems (e.g. normal distribution, poisson
distribution, bayesian linear regression...).</p></li>
<li><p>Most of the exponential family distributions lead to a closed
form of the predictive probability when the prior is conjugate, which
avoids us to resort to bayesian approximation techniques that are
computationally costly.</p></li>
</ol>
<h1 id="how-can-we-use-the-output-of-bocpd-to-detect-changes">How can we
use the output of BOCPD to detect changes?</h1>
<h2 id="an-offline-strategy-a-viterbi-like-algorithm">An offline
strategy: a viterbi-like algorithm</h2>
<p>A natural criteria to find the best segmentation up until time t is
to find the Maximum A Posteriori sequence of run-length, i.e. the most
probable sequence of run-length:<br />
<span class="math inline">\(r^{*}_{1}, ... r^{*}_{t} = arg
\max\limits_{r_{1}, ... r_{t}} P(r_{1}, ... r_{t} | x_{1}, ..., x_{t})
=  arg \max\limits_{r_{1}, ... r_{t}} P(r_{1}, ... r_{t} , x_{1}, ...,
x_{t})\)</span><br />
This optimization formulation is the same as the Viterbi Algorithm used
for Hidden Markov Models.<br />
However, the structural dependencies are slightly different, hence the
recursion is slightly different. Figure <a href="#graphical_model"
data-reference-type="ref" data-reference="graphical_model">1</a>
illustrates the structural dependencies in BOCPD.<br />
<span class="math display">\[\begin{aligned}
    P(r_{1}, ..., r_{t} , x_{1}, ..., x_{t}) =&amp; P( r_{t} | r_{1},
..., r_{t-1}, x_{1}, ..., x_{t}) P(r_{1}, ..., r_{t-1}, , x_{1}, ...,
x_{t}) \\ =&amp; P( r_{t} | r_{1}, ..., r_{t-1}, x_{1}, ..., x_{t}) P(
x_{t} | r_{1}, ..., r_{t-1}, x_{1}, ..., x_{t-1}) P(r_{1}, ..., r_{t-1}
, x_{1}, ..., x_{t-1})
    \\ =&amp; P( r_{t} | r_{t-1}) P( x_{t} | r_{t-1}, x_{1}, ...,
x_{t-1}) P(r_{1}, ..., r_{t-1} , x_{1}, ..., x_{t-1})
\end{aligned}\]</span><br />
Now let’s introduce a few notations:<br />
Let <span class="math inline">\(\delta_{t}(j) \overset{\Delta}{=}  \log
P(r_{1}, ..., r_{t}=j, x_{1}, ..., x_{t})\)</span>.<br />
Let <span class="math inline">\(A(i,j)  \overset{\Delta}{=} \log
P(r_{t}=j|r_{t-1}=i)\)</span>.<br />
Let <span class="math inline">\(\lambda(i)  \overset{\Delta}{=} \log P(
x_{t} = i| r_{t-1}, x_{1}, ..., x_{t-1})\)</span>.<br />
Using the equation above, we get:<br />
<span class="math inline">\(\delta_{t}(j) = \max\limits_{i} A(i,j) +
\lambda(i) +\delta_{t-1}(i)\)</span><br />
The recursion above allows us to compute the optimal sequence of
run-length at each time step.<br />
An important drawback is the time and space complexity of this
algorithm.<br />
The space complexity is <span class="math inline">\(O(T^{2})\)</span>
while the computational complexity is <span
class="math inline">\(O(T)\)</span><br />
.</p>
<h2 id="an-online-strategy">An online strategy</h2>
<p>In the online change-point setting, we are above all interested in
detecting a new change-point with minimum delay (more than finding
retrospectively the optimal segmentation).<br />
An easy and natural strategy would be to detect an alert if the
run-length posterior distribution <span class="math inline">\(P(r_{t} =
l| x_{1}, ..., x_{t}) &gt; \text{threshold}\)</span>.<br />
There is two parameters to consider here. The parameter <span
class="math inline">\(l\)</span> directly encodes the detection delay
with which we will flag a change-point. The threshold encodes the
confidence degree with which we will flag a change-point.<br />
One issue with this methodology is that we will generally detect
multiple consecutive change-points.</p>
<h1 id="applying-bocpd-to-detect-trend-changes">Applying BOCPD to detect
trend changes</h1>
<p>To detect trend changes, we are going to assume that</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Online detection is opposed to Offline change-point
detection. In the offline case, we are interested in identifying
structural changes after observing the entire time series. The online
case aims at identifying structural changes as new observations come
in.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A term coming from the field of survival analysis<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
